apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-stack-values
  namespace: monitoring
  labels:
    app.kubernetes.io/component: infrastructure
    app.kubernetes.io/environment: shared
    app.kubernetes.io/managed-by: flux
    app.kubernetes.io/name: infrastructure
    app.kubernetes.io/part-of: demo-app-python-assignment
    app.kubernetes.io/tier: monitoring
data:
  values.yaml: |
    # LGTM Stack Configuration
    # This deploys Loki, Grafana, Tempo, and Mimir components
    
    # Global settings
    global:
      storageClass: "local-path"  # Use local-path for Kind cluster
    
    # Loki configuration (for logs)
    loki:
      enabled: true
      persistence:
        enabled: true
        size: 10Gi
        storageClass: "local-path"
      resources:
        requests:
          memory: 128Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      service:
        type: ClusterIP
        port: 3100
    
    # Grafana configuration (for visualization)
    grafana:
      enabled: true
      adminPassword: admin
      persistence:
        enabled: true
        size: 5Gi
        storageClass: "local-path"
      service:
        type: ClusterIP
        port: 80
      resources:
        requests:
          memory: 128Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      # Enable data sources auto-configuration
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Loki
              type: loki
              url: http://lgtm-stack-loki-gateway:80
              access: proxy
              isDefault: false
            - name: Tempo
              type: tempo
              url: http://lgtm-stack-tempo-query-frontend:3100
              access: proxy
              isDefault: false
            - name: Prometheus
              type: prometheus
              url: http://lgtm-stack-mimir-nginx:80/prometheus
              access: proxy
              isDefault: true
    
    # Tempo configuration (for traces)
    tempo:
      enabled: true
      persistence:
        enabled: true
        size: 10Gi
        storageClass: "local-path"
      resources:
        requests:
          memory: 128Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      service:
        type: ClusterIP
        port: 3200
      # Disable HA for development environment
      distributor:
        replicas: 1
      querier:
        replicas: 1
      queryFrontend:
        replicas: 1
      compactor:
        replicas: 1
      # Enable OTLP receivers for OpenTelemetry data
      traces:
        otlp:
          grpc:
            enabled: true
          http:
            enabled: true
        zipkin:
          enabled: false
        jaeger:
          thriftHttp:
            enabled: false
        opencensus:
          enabled: false
    
    # Mimir configuration (for metrics - Prometheus-compatible)
    mimir:
      enabled: true
      persistence:
        enabled: true
        size: 10Gi
        storageClass: "local-path"
      resources:
        requests:
          memory: 256Mi
          cpu: 100m
        limits:
          memory: 1Gi
          cpu: 500m
    
    # Simple Prometheus server for metrics (disabled - using Mimir instead)
    prometheus:
      enabled: false
      server:
        retention: 168h
        persistentVolume:
          enabled: true
          size: 10Gi
          storageClass: "local-path"
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
          limits:
            memory: 1Gi
            cpu: 500m
        # Enable remote write receiver for Alloy
        extraArgs:
          - --web.enable-remote-write-receiver

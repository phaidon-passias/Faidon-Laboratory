apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-stack-values
  namespace: monitoring
  labels:
    app.kubernetes.io/component: infrastructure
    app.kubernetes.io/environment: shared
    app.kubernetes.io/managed-by: flux
    app.kubernetes.io/name: infrastructure
    app.kubernetes.io/part-of: demo-app-python-assignment
    app.kubernetes.io/tier: monitoring
data:
  values.yaml: |
    # LGTM Stack Configuration
    # This deploys Loki, Grafana, Tempo, and Mimir components
    
    # Global settings
    global:
      storageClass: "local-path"  # Use local-path for Kind cluster
    
    # Loki configuration (for logs)
    loki:
      enabled: true
      persistence:
        enabled: true
        size: 10Gi
        storageClass: "local-path"
      resources:
        requests:
          memory: 128Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      service:
        type: ClusterIP
        port: 3100
    
    # Grafana configuration (for visualization)
    grafana:
      enabled: true
      adminPassword: admin
      persistence:
        enabled: true
        size: 5Gi
        storageClass: "local-path"
      service:
        type: ClusterIP
        port: 80
      resources:
        requests:
          memory: 128Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      # Enable data sources auto-configuration
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Loki
              type: loki
              url: http://lgtm-stack-loki-gateway:80
              access: proxy
              isDefault: false
            - name: Tempo
              type: tempo
              url: http://lgtm-stack-tempo-query-frontend:3100
              access: proxy
              isDefault: false
            - name: Prometheus
              type: prometheus
              url: http://lgtm-stack-mimir-nginx:80/prometheus
              access: proxy
              isDefault: true
    
    # Tempo configuration (for traces)
    tempo:
      enabled: true
      persistence:
        enabled: true
        size: 10Gi
        storageClass: "local-path"
      resources:
        requests:
          memory: 128Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      service:
        type: ClusterIP
        port: 3200
      # Configure for minimal HA (2 replicas each)
      distributor:
        replicas: 2
      querier:
        replicas: 2
      queryFrontend:
        replicas: 2
      compactor:
        replicas: 2
      ingester:
        replicas: 2
      # Override Tempo configuration for single-replica mode
      config: |
        server:
          http_listen_port: 3200
          grpc_listen_port: 9095

        distributor:
          receivers:
            otlp:
              protocols:
                grpc:
                  endpoint: 0.0.0.0:4317
                http:
                  endpoint: 0.0.0.0:4318

        querier:
          frontend_worker:
            frontend_address: "lgtm-stack-tempo-query-frontend:9095"

        ingester:
          max_block_duration: 5m
          lifecycler:
            ring:
              kvstore:
                store: memberlist
              replication_factor: 1
            tokens_file_path: /var/tempo/tokens.json

        compactor:
          compaction:
            block_retention: 1h

        storage:
          trace:
            backend: local
            local:
              path: /var/tempo/traces

        memberlist:
          abort_if_cluster_join_fails: false
          bind_addr: []
          bind_port: 7946
          gossip_interval: 1s
          gossip_nodes: 2
          gossip_to_dead_nodes_time: 30s
          join_members:
          - dns+lgtm-stack-tempo-gossip-ring:7946
          leave_timeout: 5s
          left_ingesters_timeout: 5m
          max_join_backoff: 1m
          max_join_retries: 10
          min_join_backoff: 1s
          node_name: ""
          packet_dial_timeout: 5s
          packet_write_timeout: 5s
          pull_push_interval: 30s
          randomize_node_name: true
      # Enable OTLP receivers for OpenTelemetry data
      traces:
        otlp:
          grpc:
            enabled: true
          http:
            enabled: true
        zipkin:
          enabled: false
        jaeger:
          thriftHttp:
            enabled: false
        opencensus:
          enabled: false
    
    # Mimir configuration (for metrics - Prometheus-compatible)
    mimir:
      enabled: true
      persistence:
        enabled: true
        size: 10Gi
        storageClass: "local-path"
      resources:
        requests:
          memory: 256Mi
          cpu: 100m
        limits:
          memory: 1Gi
          cpu: 500m
    
    # Simple Prometheus server for metrics (disabled - using Mimir instead)
    prometheus:
      enabled: false
      server:
        retention: 168h
        persistentVolume:
          enabled: true
          size: 10Gi
          storageClass: "local-path"
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
          limits:
            memory: 1Gi
            cpu: 500m
        # Enable remote write receiver for Alloy
        extraArgs:
          - --web.enable-remote-write-receiver
